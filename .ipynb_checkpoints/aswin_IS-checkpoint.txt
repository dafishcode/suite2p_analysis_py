import numpy as np 
import scipy.stats as stat
import matplotlib.pyplot as plt





#Functional form for power law
def powerlaw(n,lam):
    zeta=np.sum(1.0/np.arange(a,b+1)**lam)
    return(n**(-lam)/zeta)

#Functional form for lognormal
def lognormal(n,mu,sig):
    return(1.0/n/np.sqrt(2*np.pi*sig**2)*np.exp(-(np.log(n)-mu)**2/(2*sig**2)))


#Calculate loglikelihood for power law given the data
#Likelihoods across all prior distribution draws, given your data
def LogLikelihood(lam):
    #normalisation factor for all lambda draw - normalises a distribution to sum of probability = 1
    zetamat=np.power.outer(1.0/np.arange(a,b+1),lam) #Matrix of normalisation constants for each lambda draw: each row =  size**-current lambda, for every size from max to min
    zeta=np.sum(zetamat,0) #Norm vector - sum of norm constants for entire max-min range at each lambda
    norm=-M*np.log(zeta) #Contribution of zeta to the likelihood
    nprod=-lam*np.sum(np.log(sizes)) #Loglikelihood calculation, given the data
    loglik=nprod+norm #Normalised loglikelihood
    return(loglik) 

#Calculate loglikelihood for lognormal
#Likelihoods across all prior distribution draws, given your data
def LogLikelihood_LN(mu,sig):
    T1 = -np.sum(np.log(sizes))
    T2_mat = np.subtract.outer(np.log(sizes),mu)**2
    T2 = -np.sum(T2_mat,0)/(2*sig**2)
    T0 = -M*np.log(np.sqrt(2*np.pi) * sig )
    loglik=T0+T1+T2
    return(loglik) 


#IMPORTANCE SAMPLER - for power law
#OUTPUT - posterior average exponent, log marginal likelihood, effective sample size = how good is the sampler
def IS(npart):
    lambda_sample=np.random.uniform(0.1,5,npart) #randomly sample 

    #Weights - combine the expected parameter value from the prior (normal) and proposal distributions (uniform), weighted by the likelihood 
    #Prior value*likelihood divided by proposal value - cancel out the effect of the proposal
    weights=LogLikelihood(lambda_sample)+stat.norm.logpdf(lambda_sample,1,3)-stat.uniform.logpdf(lambda_sample,0.1,5) #Log of weights
    maxw=np.max(weights)
    w2 = np.exp(weights-maxw)
    w2_sum = np.sum(w2)
    ESS=1.0/(np.sum((w2/w2_sum)**2))
    mean_lambda = np.dot(lambda_sample,w2)/w2_sum #average of the lambda value for the posterior distribution
    #marginal likelihood = empirical means of all the weights
    marglik = maxw + np.log(np.sum(np.exp(weights-maxw)))-np.log(npart) #Take the exponent of logs to unlog, before summing/then divded by ncounts
    return([mean_lambda, marglik, LogLikelihood(lambda_sample), ESS])

#IMPORTANCE SAMPLER - for lognormal
#OUTPUT - posterior average exponent, log marginal likelihood, effective sample size = how good is the sampler
def IS_LN(npart):
    mu_sample = np.random.uniform(-2.0,2.0,npart) #randomly sample mu
    sig_sample = np.random.uniform(0.1,5.0,npart) #randomly sample sigma
    weights=LogLikelihood_LN(mu_sample,sig_sample)
    maxw=np.max(weights)
    w2 = np.exp(weights-maxw)
    w2_sum = np.sum(w2)
    ESS=1.0/(np.sum((w2/w2_sum)**2))
    wmax_ID=np.argmax(w2)
    mean_mu = mu_sample[wmax_ID]
    mean_sig = sig_sample[wmax_ID]
    #marginal likelihood = empirical means of all the weights
    marglik = maxw + np.log(np.sum(np.exp(weights-maxw)))-np.log(npart)
    return([mean_mu,mean_sig, marglik,  LogLikelihood_LN(mu_sample, sig_sample), ESS])

#find entire posterior dsitr - posterior distribution - range of values 
def plot_samples(npart):
    lambda_sample=np.random.uniform(0.1,5,npart)
    weights=LogLikelihood(lambda_sample)
    maxw=np.max(weights)
    w2 = np.exp(weights-maxw)
    plt.hist(lambda_sample,weights=w2,bins=np.linspace(2.5,2.8))
    plt.show()

def plotcomp(lam,mu,sig):
    x = np.linspace(a,b,40) 
    plt.hist(sizes,40,log=True,density=True)
    plt.plot(x,powerlaw(x,lam))
    plt.plot(x,lognormal(x,mu,sig))
    plt.show()
    
#RUN 
sizes=data
M=len(sizes)
a=min(sizes) #define xmin
b=max(sizes) #define xmax
npart = 2000 #number of particles - number of draws from prior distribution
ln=IS_LN(npart)
po=IS(npart)
